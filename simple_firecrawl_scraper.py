#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆAIå·¥å…·æ•°æ®æŠ“å–å™¨
ç›´æ¥æŠ“å–ç½‘ç«™ä¿¡æ¯ï¼Œä¸ä¾èµ–å¤–éƒ¨æ¨¡å—
"""

import requests
import json
import time
import csv
import os
from typing import Dict, List, Optional

class SimpleFirecrawlScraper:
    """ç®€åŒ–çš„FirecrawlæŠ“å–å™¨"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.firecrawl.dev"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def load_csv_data(self, file_path: str) -> List[Dict]:
        """åŠ è½½CSVæ•°æ®"""
        tools_data = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                
                for row in reader:
                    # æ‰¾åˆ°äº§å“åç§°å’ŒURLåˆ—
                    product_name = None
                    product_url = None
                    category = None
                    
                    for key, value in row.items():
                        if value and value.strip():
                            # åˆ¤æ–­æ˜¯å¦ä¸ºURL
                            if value.startswith('http'):
                                product_url = value.strip()
                                # ä»åˆ—åæ¨æ–­ç±»åˆ«
                                if 'äº§å“åç§°' in key or 'product' in key.lower():
                                    continue
                                # ä»åˆ—åæ¨æ–­ç±»åˆ«
                                for cat in ['AI Search Engine', 'AI ChatBots', 'AI Character Generator', 
                                          'AI Presentation Maker', 'AI Image Generator', 'AI Image Editor',
                                          'AI Image Enhancer', 'AI Video Generator', 'AI Video Editing', 
                                          'AI Music Generator']:
                                    if cat in key:
                                        category = cat
                                        break
                            else:
                                # å¯èƒ½æ˜¯äº§å“åç§°
                                if not product_name:
                                    product_name = value.strip()
                    
                    if product_name and product_url:
                        tools_data.append({
                            'product_name': product_name,
                            'product_url': product_url,
                            'original_category_name': category or 'Unknown'
                        })
            
            return tools_data
            
        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def load_schema(self) -> Dict:
        """åŠ è½½æŠ“å–Schema"""
        try:
            with open('ai_tool_firecrawl_schema.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½Schemaï¼Œä½¿ç”¨é»˜è®¤Schema: {e}")
            # è¿”å›ç®€åŒ–çš„é»˜è®¤Schema
            return {
                "type": "object",
                "properties": {
                    "product_name": {"type": "string", "description": "äº§å“åç§°"},
                    "short_introduction": {"type": "string", "description": "ç®€çŸ­ä»‹ç»"},
                    "logo_img_url": {"type": "string", "description": "Logoå›¾ç‰‡URL"},
                    "overview_img_url": {"type": "string", "description": "æ¦‚è§ˆå›¾ç‰‡URL"},
                    "primary_task": {"type": "string", "description": "ä¸»è¦åŠŸèƒ½"},
                    "author_company": {"type": "string", "description": "å¼€å‘å…¬å¸"},
                    "general_price_tag": {"type": "string", "description": "ä»·æ ¼æ ‡ç­¾"}
                }
            }
    
    def scrape_tool(self, url: str, schema: Dict) -> Optional[Dict]:
        """æŠ“å–å•ä¸ªå·¥å…·"""
        try:
            payload = {
                "url": url,
                "formats": ["extract"],
                "extract": {"schema": schema}
            }
            
            response = requests.post(
                f"{self.base_url}/v1/scrape",
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if data.get("success") and "extract" in data:
                    return data["extract"]
            
            print(f"âš ï¸ æŠ“å–å¤±è´¥ {response.status_code}: {url}")
            return None
            
        except Exception as e:
            print(f"âŒ æŠ“å–å¼‚å¸¸: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    
    print("\n=== ç®€åŒ–ç‰ˆAIå·¥å…·æ•°æ®æŠ“å–å™¨ ===")
    
    # é…ç½®
    api_key = "fc-4a9ca9115114472d92ace77332cf0262"  # æ‚¨çš„Firecrawl APIå¯†é’¥
    csv_file = "AIå·¥å…·æ±‡æ€»-å·¥ä½œè¡¨2.csv"
    max_tools = 3  # æµ‹è¯•é™åˆ¶
    
    # åˆå§‹åŒ–æŠ“å–å™¨
    scraper = SimpleFirecrawlScraper(api_key)
    
    # åŠ è½½Schema
    schema = scraper.load_schema()
    print(f"âœ… SchemaåŠ è½½å®Œæˆï¼ŒåŒ…å« {len(schema.get('properties', {}))} ä¸ªå­—æ®µ")
    
    # åŠ è½½CSVæ•°æ®
    print(f"\nğŸ“ åŠ è½½CSVæ–‡ä»¶: {csv_file}")
    tools_data = scraper.load_csv_data(csv_file)
    
    if not tools_data:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å·¥å…·æ•°æ®")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(tools_data)} ä¸ªAIå·¥å…·")
    
    # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆæµ‹è¯•ï¼‰
    if max_tools and len(tools_data) > max_tools:
        tools_data = tools_data[:max_tools]
        print(f"ğŸ”§ æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶å¤„ç† {len(tools_data)} ä¸ªå·¥å…·")
    
    # æ˜¾ç¤ºå°†è¦å¤„ç†çš„å·¥å…·
    print(f"\nğŸ“‹ å°†è¦æŠ“å–çš„å·¥å…·:")
    for i, tool in enumerate(tools_data, 1):
        print(f"  {i}. {tool['product_name']} - {tool['product_url']}")
    
    # å¼€å§‹æŠ“å–
    print(f"\nğŸš€ å¼€å§‹æŠ“å–...\n")
    
    results = []
    success_count = 0
    
    for index, tool in enumerate(tools_data, 1):
        print(f"ğŸ“¦ [{index}/{len(tools_data)}] {tool['product_name']}")
        print(f"ğŸ”— {tool['product_url']}")
        
        # æŠ“å–æ•°æ®
        scraped_data = scraper.scrape_tool(tool['product_url'], schema)
        
        # åˆå¹¶æ•°æ®
        merged = tool.copy()
        if scraped_data:
            success_count += 1
            merged['scraped_successfully'] = True
            merged['scraped_data'] = scraped_data
            print(f"âœ… æŠ“å–æˆåŠŸï¼Œè·å¾— {len(scraped_data)} ä¸ªå­—æ®µ")
            
            # æ˜¾ç¤ºæŠ“å–åˆ°çš„å…³é”®å­—æ®µ
            key_fields = ['product_name', 'short_introduction', 'primary_task', 'author_company']
            for field in key_fields:
                if field in scraped_data and scraped_data[field]:
                    print(f"   {field}: {scraped_data[field][:50]}{'...' if len(str(scraped_data[field])) > 50 else ''}")
        else:
            merged['scraped_successfully'] = False
            merged['scraped_data'] = None
            print("âŒ æŠ“å–å¤±è´¥")
        
        results.append(merged)
        
        # å»¶è¿Ÿ
        if index < len(tools_data):
            print("â³ ç­‰å¾…2ç§’...\n")
            time.sleep(2)
    
    # ä¿å­˜ç»“æœ
    output_file = "scraped_results.json"
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡
    print(f"\nğŸ“Š æŠ“å–ç»Ÿè®¡:")
    print(f"âœ… æ€»è®¡: {len(tools_data)}")
    print(f"âœ… æˆåŠŸ: {success_count}")
    print(f"âŒ å¤±è´¥: {len(tools_data) - success_count}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_count/len(tools_data)*100:.1f}%")
    
    print(f"\nâœ¨ å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")
    print("æ‚¨å¯ä»¥æŸ¥çœ‹æŠ“å–ç»“æœï¼Œç¡®è®¤åå†è¿›è¡ŒWordPresså¯¼å…¥ã€‚")

if __name__ == "__main__":
    main() 