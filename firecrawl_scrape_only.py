#!/usr/bin/env python3
"""
AIå·¥å…·æ•°æ®æŠ“å–å™¨ - ä½¿ç”¨FirecrawlæŠ“å–ç½‘ç«™ä¿¡æ¯å¹¶åˆå¹¶CSVæ•°æ®
ä»…æŠ“å–å’Œåˆå¹¶ï¼Œä¸è¿›è¡ŒWordPresså¯¼å…¥
"""

import requests
import json
import time
import logging
from typing import Dict, List, Optional
from csv_data_processor import CSVDataProcessor
from config import Config

class FirecrawlScraper:
    """FirecrawlæŠ“å–å™¨"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.firecrawl.dev"):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('firecrawl_scrape.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_schema(self) -> Dict:
        """åŠ è½½æŠ“å–Schema"""
        try:
            with open('ai_tool_firecrawl_schema.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"æ— æ³•åŠ è½½Schema: {e}")
            return {}
    
    def scrape_single_tool(self, url: str, schema: Dict) -> Optional[Dict]:
        """æŠ“å–å•ä¸ªå·¥å…·çš„æ•°æ®"""
        try:
            payload = {
                "url": url,
                "formats": ["extract"],
                "extract": {
                    "schema": schema
                }
            }
            
            self.logger.info(f"ğŸ” æ­£åœ¨æŠ“å–: {url}")
            
            response = requests.post(
                f"{self.base_url}/v1/scrape",
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if data.get("success") and "extract" in data:
                    extracted = data["extract"]
                    self.logger.info(f"âœ… æŠ“å–æˆåŠŸï¼Œè·å¾— {len(extracted)} ä¸ªå­—æ®µ")
                    return extracted
                else:
                    self.logger.warning(f"âš ï¸ æŠ“å–æˆåŠŸä½†æ— æå–æ•°æ®: {url}")
                    return None
            else:
                self.logger.error(f"âŒ æŠ“å–å¤±è´¥ {response.status_code}: {url}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ æŠ“å–å¼‚å¸¸ {url}: {e}")
            return None
    
    def merge_data(self, csv_data: Dict, scraped_data: Optional[Dict]) -> Dict:
        """åˆå¹¶CSVæ•°æ®å’ŒæŠ“å–æ•°æ®"""
        merged = csv_data.copy()
        
        if scraped_data:
            # æ·»åŠ æŠ“å–æ ‡è®°
            merged['scraped_successfully'] = True
            merged['scraped_fields_count'] = len(scraped_data)
            
            # åˆå¹¶æŠ“å–çš„å­—æ®µ
            for key, value in scraped_data.items():
                if value is not None and value != "":
                    merged[f"scraped_{key}"] = value
                    
            # ç‰¹æ®Šå¤„ç†å›¾ç‰‡URL
            if "logo_img_url" in scraped_data:
                merged["logo_img_url"] = scraped_data["logo_img_url"]
            if "overview_img_url" in scraped_data:
                merged["overview_img_url"] = scraped_data["overview_img_url"]
        else:
            merged['scraped_successfully'] = False
            merged['scraped_fields_count'] = 0
        
        return merged

def main():
    """ä¸»å‡½æ•°"""
    
    # åŠ è½½é…ç½®
    config = Config()
    
    print("\n=== AIå·¥å…·æ•°æ®æŠ“å–å™¨ ===")
    print("ä»…è¿›è¡ŒFirecrawlæŠ“å–å’Œæ•°æ®åˆå¹¶ï¼Œä¸è¿›è¡ŒWordPresså¯¼å…¥\n")
    
    # æ£€æŸ¥Firecrawlé…ç½®
    if not config.firecrawl_api_key:
        print("âŒ é”™è¯¯ï¼šæœªé…ç½®FIRECRAWL_API_KEY")
        print("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„Firecrawl APIå¯†é’¥")
        return
    
    # åˆå§‹åŒ–æŠ“å–å™¨
    scraper = FirecrawlScraper(config.firecrawl_api_key, config.firecrawl_base_url)
    schema = scraper.load_schema()
    
    if not schema:
        print("âŒ é”™è¯¯ï¼šæ— æ³•åŠ è½½Firecrawl Schema")
        return
    
    print(f"âœ… Firecrawlé…ç½®æ­£å¸¸ï¼ŒSchemaåŒ…å« {len(schema.get('properties', {}))} ä¸ªå­—æ®µ")
    
    # åŠ è½½CSVæ•°æ®
    print("\nğŸ“ åŠ è½½CSVæ•°æ®...")
    processor = CSVDataProcessor(config.csv_file_path)
    tools_data = processor.get_tools_data()
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(tools_data)} ä¸ªAIå·¥å…·")
    
    # å¤„ç†æ•°é‡é™åˆ¶
    if config.debug_mode and config.max_tools_to_process:
        tools_data = tools_data[:config.max_tools_to_process]
        print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šé™åˆ¶å¤„ç† {len(tools_data)} ä¸ªå·¥å…·")
    
    # å¼€å§‹æŠ“å–
    print(f"\nğŸš€ å¼€å§‹æŠ“å– {len(tools_data)} ä¸ªå·¥å…·çš„è¯¦ç»†ä¿¡æ¯...\n")
    
    merged_results = []
    success_count = 0
    failed_count = 0
    
    for index, tool in enumerate(tools_data, 1):
        tool_name = tool.get('product_name', 'Unknown')
        tool_url = tool.get('product_url', '')
        
        print(f"ğŸ“¦ [{index}/{len(tools_data)}] å¤„ç†: {tool_name}")
        print(f"ğŸ”— URL: {tool_url}")
        
        if not tool_url:
            print("âš ï¸ è·³è¿‡ï¼šæ— URL")
            merged = scraper.merge_data(tool, None)
            failed_count += 1
        else:
            # æŠ“å–æ•°æ®
            scraped_data = scraper.scrape_single_tool(tool_url, schema)
            
            # åˆå¹¶æ•°æ®
            merged = scraper.merge_data(tool, scraped_data)
            
            if scraped_data:
                success_count += 1
                print(f"âœ… æˆåŠŸæŠ“å– {len(scraped_data)} ä¸ªå­—æ®µ")
            else:
                failed_count += 1
                print("âŒ æŠ“å–å¤±è´¥")
        
        merged_results.append(merged)
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
        if index < len(tools_data):
            print("â³ ç­‰å¾…2ç§’...\n")
            time.sleep(2)
    
    # ä¿å­˜ç»“æœ
    output_file = "merged_ai_tools_data.json"
    
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœåˆ°: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_results, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ“Š æŠ“å–å®Œæˆç»Ÿè®¡:")
    print(f"âœ… æ€»è®¡å·¥å…·: {len(tools_data)}")
    print(f"âœ… æŠ“å–æˆåŠŸ: {success_count}")
    print(f"âŒ æŠ“å–å¤±è´¥: {failed_count}")
    print(f"ğŸ“Š æˆåŠŸç‡: {success_count/len(tools_data)*100:.1f}%")
    
    # æ˜¾ç¤ºæ ·ä¾‹æ•°æ®
    if merged_results:
        print(f"\nğŸ” ç¬¬ä¸€ä¸ªå·¥å…·çš„åˆå¹¶æ•°æ®æ ·ä¾‹:")
        sample = merged_results[0]
        print(f"äº§å“åç§°: {sample.get('product_name', 'N/A')}")
        print(f"æŠ“å–æˆåŠŸ: {sample.get('scraped_successfully', 'N/A')}")
        print(f"æŠ“å–å­—æ®µæ•°: {sample.get('scraped_fields_count', 'N/A')}")
        
        # æ˜¾ç¤ºæŠ“å–åˆ°çš„å­—æ®µ
        scraped_fields = [k for k in sample.keys() if k.startswith('scraped_')]
        if scraped_fields:
            print(f"æŠ“å–å­—æ®µ: {', '.join(scraped_fields[:5])}{'...' if len(scraped_fields) > 5 else ''}")
    
    print(f"\nâœ¨ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    print("æ‚¨å¯ä»¥æŸ¥çœ‹åˆå¹¶ç»“æœï¼Œç¡®è®¤åå†è¿›è¡ŒWordPresså¯¼å…¥ã€‚")

if __name__ == "__main__":
    main() 