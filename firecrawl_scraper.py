"""
AIå·¥å…·å¯¼å…¥ç³»ç»Ÿ - Firecrawlæ•°æ®æŠ“å–å™¨
"""

import requests
import json
import time
from config import config
from logger import logger

class FirecrawlScraper:
    """Firecrawlç½‘ç«™æ•°æ®æŠ“å–å™¨"""
    
    def __init__(self):
        self.api_key = config.FIRECRAWL_API_KEY
        self.timeout = config.FIRECRAWL_TIMEOUT
        self.request_count = 0
        self.last_request_time = 0
        self.min_delay = 6.5  # å…è´¹è®¡åˆ’: 60ç§’/10æ¬¡ = 6ç§’é—´éš”ï¼ŒåŠ 0.5ç§’ç¼“å†²
        
        if not self.api_key:
            raise ValueError("Firecrawl APIå¯†é’¥æœªé…ç½®")
        
    def load_schema(self):
        """åŠ è½½æŠ“å–Schema"""
        try:
            with open(config.SCHEMA_FILE, 'r', encoding='utf-8') as f:
                schema = json.load(f)
            logger.debug(f"æˆåŠŸåŠ è½½Schema: {config.SCHEMA_FILE}")
            return schema
        except FileNotFoundError:
            logger.error(f"Schemaæ–‡ä»¶ä¸å­˜åœ¨: {config.SCHEMA_FILE}")
            return None
        except json.JSONDecodeError:
            logger.error(f"Schemaæ–‡ä»¶æ ¼å¼é”™è¯¯: {config.SCHEMA_FILE}")
            return None
        
    def _rate_limit_delay(self):
        """å®ç°é€Ÿç‡é™åˆ¶å»¶è¿Ÿ"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_delay:
            sleep_time = self.min_delay - time_since_last
            logger.info(f"ğŸ• é€Ÿç‡é™åˆ¶å»¶è¿Ÿ {sleep_time:.1f} ç§’...")
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
        self.request_count += 1
    
    def _check_credits(self):
        """æ£€æŸ¥å‰©ä½™creditsï¼ˆå¦‚æœAPIæ”¯æŒï¼‰"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ creditsæ£€æŸ¥çš„APIè°ƒç”¨
            # ç›®å‰å…ˆè®°å½•è¯·æ±‚æ•°
            if self.request_count > 0 and self.request_count % 10 == 0:
                logger.warning(f"âš ï¸  å·²ä½¿ç”¨ {self.request_count} æ¬¡APIè°ƒç”¨ï¼Œè¯·æ³¨æ„å…è´¹é¢åº¦é™åˆ¶")
        except Exception as e:
            logger.debug(f"æ— æ³•æ£€æŸ¥credits: {e}")
    
    def scrape_website(self, url, schema):
        """æŠ“å–å•ä¸ªç½‘ç«™æ•°æ®"""
        try:
            # å®æ–½é€Ÿç‡é™åˆ¶
            self._rate_limit_delay()
            
            # æ£€æŸ¥credits
            self._check_credits()
            
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            payload = {
                'url': url,
                'formats': ['extract'],
                'extract': {
                    'schema': schema,
                    'systemPrompt': 'è¯·ä¸¥æ ¼æŒ‰ç…§æä¾›çš„schemaæ ¼å¼æå–ç½‘ç«™ä¿¡æ¯ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼ã€‚å¦‚æœæŸä¸ªå­—æ®µæ— æ³•ä»ç½‘ç«™è·å–ï¼Œè¯·æä¾›åˆç†çš„é»˜è®¤å€¼ã€‚'
                }
            }
            
            logger.debug(f"æ­£åœ¨æŠ“å–ç½‘ç«™: {url}")
            
            response = requests.post(
                'https://api.firecrawl.dev/v1/scrape',
                headers=headers,
                json=payload,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success') and result.get('data', {}).get('extract'):
                    extract_data = result['data']['extract']
                    logger.success(f"âœ“ æŠ“å–æˆåŠŸ: {url}")
                    return extract_data
                else:
                    logger.warning(f"æŠ“å–è¿”å›ç©ºæ•°æ®: {url}")
                    return None
            elif response.status_code == 402:
                logger.error(f"ğŸ’³ Firecrawl APIé¢åº¦ä¸è¶³æˆ–è¶…è¿‡é€Ÿç‡é™åˆ¶: {url}")
                logger.error("ğŸ“Š å¯èƒ½çš„åŸå› :")
                logger.error("   â€¢ å…è´¹è®¡åˆ’500 creditså·²ç”¨å®Œ")
                logger.error("   â€¢ è¶…è¿‡æ¯åˆ†é’Ÿ10æ¬¡æŠ“å–é™åˆ¶")
                logger.error("   â€¢ éœ€è¦å‡çº§ä»˜è´¹è®¡åˆ’")
                logger.error("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                logger.error("   1. ç­‰å¾…ä¸‹ä¸ªæœˆé¢åº¦é‡ç½®")
                logger.error("   2. å‡çº§åˆ°ä»˜è´¹è®¡åˆ’")
                logger.error("   3. ä½¿ç”¨ ENABLE_FIRECRAWL=false ç¦ç”¨æŠ“å–")
                return None
            else:
                logger.error(f"æŠ“å–å¤±è´¥ {response.status_code}: {url}")
                if response.status_code == 429:
                    logger.warning("é€Ÿç‡é™åˆ¶ï¼Œå»ºè®®å¢åŠ å»¶è¿Ÿæ—¶é—´")
                return None
                
        except requests.exceptions.Timeout:
            logger.error(f"â±ï¸  è¯·æ±‚è¶…æ—¶: {url}")
            return None
        except Exception as e:
            logger.error(f"æŠ“å–å¼‚å¸¸ {url}: {e}")
            return None
    
    def scrape_single(self, tool_data, schema, max_retries=3):
        """æŠ“å–å•ä¸ªç½‘ç«™æ•°æ®ï¼ŒåŒ…å«é‡è¯•å’Œé”™è¯¯å¤„ç†"""
        url = tool_data.get('url', '').strip()
        
        if not url:
            return {
                'status': 'error',
                'message': 'URLä¸ºç©º',
                'data': tool_data
            }
        
        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        for attempt in range(max_retries):
            try:
                logger.debug(f"å°è¯•æŠ“å– ({attempt + 1}/{max_retries}): {url}")
                
                # æŠ“å–æ•°æ®
                scraped_data = self.scrape_website(url, schema)
                
                if scraped_data:
                    # ç¡®ä¿åŸºæœ¬å­—æ®µå­˜åœ¨
                    if not scraped_data.get('product_name'):
                        scraped_data['product_name'] = tool_data.get('product_name', 'Unknown')
                    
                    if not scraped_data.get('product_url'):
                        scraped_data['product_url'] = url
                    
                    if not scraped_data.get('category'):
                        scraped_data['category'] = tool_data.get('category', 'AI Tools')
                    
                    # æ·»åŠ åŸå§‹åˆ†ç±»ä¿¡æ¯
                    scraped_data['original_category_name'] = tool_data.get('category', '')
                    
                    logger.success(f"âœ“ æŠ“å–æˆåŠŸ: {scraped_data.get('product_name', 'Unknown')}")
                    return {
                        'status': 'success',
                        'data': scraped_data
                    }
                else:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 5
                        logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    
            except Exception as e:
                error_msg = str(e)
                logger.error(f"âŒ æŠ“å–å¤±è´¥: {error_msg}")
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"ğŸ’¥ æ‰€æœ‰é‡è¯•å¤±è´¥: {url}")
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œè¿”å›é”™è¯¯
        return {
            'status': 'error',
            'message': f'æŠ“å–å¤±è´¥: {url}',
            'data': tool_data
        }

    def scrape_batch(self, tool_list, schema):
        """æ‰¹é‡æŠ“å–å·¥å…·æ•°æ®"""
        results = []
        total = len(tool_list)
        
        logger.info(f"å¼€å§‹æ‰¹é‡æŠ“å– {total} ä¸ªå·¥å…·")
        
        for i, tool in enumerate(tool_list, 1):
            logger.info(f"[{i}/{total}] æŠ“å–: {tool['product_name']}")
            
            # ä½¿ç”¨å•ä¸ªæŠ“å–æ–¹æ³•
            result = self.scrape_single(tool, schema)
            results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿ
            if i < total:
                time.sleep(self.min_delay)
        
        success_count = sum(1 for r in results if r['status'] == 'success')
        logger.info(f"æ‰¹é‡æŠ“å–å®Œæˆ: {success_count}/{total} æˆåŠŸ")
        
        return results 