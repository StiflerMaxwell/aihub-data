#!/usr/bin/env python3
"""
AIå·¥å…·FirecrawlæŠ“å–å™¨ - æœ€ç»ˆç‰ˆæœ¬
ä½¿ç”¨å·²å¤„ç†çš„JSONæ•°æ®è¿›è¡ŒæŠ“å–
"""

import requests
import json
import time
from typing import Dict, List, Optional

class FinalFirecrawlScraper:
    """æœ€ç»ˆç‰ˆFirecrawlæŠ“å–å™¨"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.firecrawl.dev"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def load_tools_data(self) -> List[Dict]:
        """åŠ è½½å·²å¤„ç†çš„å·¥å…·æ•°æ®"""
        try:
            with open('ai_tools_data.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½å·¥å…·æ•°æ®å¤±è´¥: {e}")
            return []
    
    def load_schema(self) -> Dict:
        """åŠ è½½æŠ“å–Schema"""
        try:
            with open('ai_tool_firecrawl_schema.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å®Œæ•´Schemaï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {e}")
            # ç®€åŒ–çš„Schema
            return {
                "type": "object",
                "properties": {
                    "short_introduction": {
                        "type": "string",
                        "description": "äº§å“çš„ç®€çŸ­ä»‹ç»æˆ–æè¿°"
                    },
                    "logo_img_url": {
                        "type": "string", 
                        "description": "äº§å“Logoå›¾ç‰‡çš„URLåœ°å€"
                    },
                    "overview_img_url": {
                        "type": "string",
                        "description": "äº§å“æ¦‚è§ˆå›¾ç‰‡æˆ–æˆªå›¾çš„URLåœ°å€"
                    },
                    "primary_task": {
                        "type": "string",
                        "description": "äº§å“çš„ä¸»è¦åŠŸèƒ½æˆ–ç”¨é€”"
                    },
                    "author_company": {
                        "type": "string",
                        "description": "å¼€å‘è¿™ä¸ªäº§å“çš„å…¬å¸æˆ–ä½œè€…åç§°"
                    },
                    "general_price_tag": {
                        "type": "string",
                        "description": "äº§å“çš„ä»·æ ¼ä¿¡æ¯ï¼Œå¦‚å…è´¹ã€ä»˜è´¹ã€è®¢é˜…ç­‰"
                    }
                }
            }
    
    def scrape_tool(self, url: str, schema: Dict) -> Optional[Dict]:
        """æŠ“å–å•ä¸ªå·¥å…·çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            payload = {
                "url": url,
                "formats": ["extract"],
                "extract": {"schema": schema}
            }
            
            response = requests.post(
                f"{self.base_url}/v1/scrape",
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if data.get("success") and "extract" in data:
                    return data["extract"]
                else:
                    print(f"   âš ï¸ æŠ“å–æˆåŠŸä½†æœªæå–åˆ°æ•°æ®")
                    return None
            else:
                print(f"   âŒ æŠ“å–å¤±è´¥ {response.status_code}")
                if response.status_code == 402:
                    print("   ğŸ’³ APIé…é¢ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ‚¨çš„Firecrawlè´¦æˆ·")
                return None
                
        except Exception as e:
            print(f"   âŒ æŠ“å–å¼‚å¸¸: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    
    print("\n=== AIå·¥å…·FirecrawlæŠ“å–å™¨ - æœ€ç»ˆç‰ˆæœ¬ ===")
    print("ä½¿ç”¨å·²å¤„ç†çš„JSONæ•°æ®è¿›è¡Œç½‘ç«™å†…å®¹æŠ“å–\n")
    
    # é…ç½®
    api_key = "fc-4a9ca9115114472d92ace77332cf0262"
    max_tools = 5  # é™åˆ¶æµ‹è¯•æ•°é‡
    
    # åˆå§‹åŒ–æŠ“å–å™¨
    scraper = FinalFirecrawlScraper(api_key)
    
    # åŠ è½½Schema
    schema = scraper.load_schema()
    print(f"âœ… SchemaåŠ è½½å®Œæˆï¼ŒåŒ…å« {len(schema.get('properties', {}))} ä¸ªæŠ“å–å­—æ®µ")
    
    # åŠ è½½å·¥å…·æ•°æ®
    print("ğŸ“ åŠ è½½å·²å¤„ç†çš„å·¥å…·æ•°æ®...")
    tools_data = scraper.load_tools_data()
    
    if not tools_data:
        print("âŒ æœªæ‰¾åˆ°å·¥å…·æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ csv_data_processor.py")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(tools_data)} ä¸ªAIå·¥å…·")
    
    # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆæµ‹è¯•ï¼‰
    if max_tools and len(tools_data) > max_tools:
        tools_data = tools_data[:max_tools]
        print(f"ğŸ”§ æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶å¤„ç†å‰ {len(tools_data)} ä¸ªå·¥å…·")
    
    # æ˜¾ç¤ºå°†è¦æŠ“å–çš„å·¥å…·
    print(f"\nğŸ“‹ å°†è¦æŠ“å–çš„å·¥å…·:")
    for i, tool in enumerate(tools_data, 1):
        print(f"  {i}. {tool['product_name']} ({tool['category']})")
        print(f"     ğŸ”— {tool['url']}")
    
    # ç¡®è®¤ç»§ç»­
    print(f"\nå‡†å¤‡å¼€å§‹æŠ“å–ï¼Œæ¯ä¸ªå·¥å…·é—´éš”2ç§’...")
    input("æŒ‰å›è½¦é”®ç»§ç»­ï¼Œæˆ–Ctrl+Cå–æ¶ˆ...")
    
    # å¼€å§‹æŠ“å–
    print(f"\nğŸš€ å¼€å§‹æŠ“å–...\n")
    
    results = []
    success_count = 0
    total_fields_scraped = 0
    
    for index, tool in enumerate(tools_data, 1):
        print(f"ğŸ“¦ [{index}/{len(tools_data)}] {tool['product_name']}")
        print(f"ğŸ”— {tool['url']}")
        print(f"ğŸ“‚ ç±»åˆ«: {tool['category']}")
        
        # æŠ“å–æ•°æ®
        scraped_data = scraper.scrape_tool(tool['url'], schema)
        
        # åˆå¹¶æ•°æ®
        merged = tool.copy()
        
        if scraped_data:
            success_count += 1
            fields_count = len([v for v in scraped_data.values() if v])
            total_fields_scraped += fields_count
            
            merged['scraped_successfully'] = True
            merged['scraped_data'] = scraped_data
            merged['scraped_fields_count'] = fields_count
            
            print(f"   âœ… æŠ“å–æˆåŠŸï¼Œè·å¾— {fields_count} ä¸ªæœ‰æ•ˆå­—æ®µ")
            
            # æ˜¾ç¤ºæŠ“å–åˆ°çš„å…³é”®ä¿¡æ¯
            if scraped_data.get('short_introduction'):
                intro = scraped_data['short_introduction'][:80]
                print(f"   ğŸ“ ç®€ä»‹: {intro}{'...' if len(scraped_data['short_introduction']) > 80 else ''}")
            
            if scraped_data.get('primary_task'):
                print(f"   ğŸ¯ åŠŸèƒ½: {scraped_data['primary_task']}")
            
            if scraped_data.get('author_company'):
                print(f"   ğŸ¢ å…¬å¸: {scraped_data['author_company']}")
            
            if scraped_data.get('logo_img_url'):
                print(f"   ğŸ–¼ï¸ Logo: {scraped_data['logo_img_url']}")
        else:
            merged['scraped_successfully'] = False
            merged['scraped_data'] = None
            merged['scraped_fields_count'] = 0
            print("   âŒ æŠ“å–å¤±è´¥æˆ–æ— æ•°æ®")
        
        results.append(merged)
        
        # è¿›åº¦æ˜¾ç¤º
        progress = (index / len(tools_data)) * 100
        print(f"   ğŸ“Š è¿›åº¦: {progress:.1f}% ({success_count}/{index} æˆåŠŸ)")
        
        # å»¶è¿Ÿ
        if index < len(tools_data):
            print("   â³ ç­‰å¾…2ç§’...\n")
            time.sleep(2)
    
    # ä¿å­˜ç»“æœ
    output_file = "scraped_ai_tools_complete.json"
    print(f"\nğŸ’¾ ä¿å­˜å®Œæ•´ç»“æœåˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ä»…æŠ“å–æˆåŠŸçš„æ•°æ®
    successful_results = [r for r in results if r.get('scraped_successfully')]
    if successful_results:
        success_file = "scraped_ai_tools_success_only.json"
        with open(success_file, 'w', encoding='utf-8') as f:
            json.dump(successful_results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¿å­˜æˆåŠŸæ•°æ®åˆ°: {success_file}")
    
    # è¯¦ç»†ç»Ÿè®¡
    print(f"\nğŸ“Š æŠ“å–å®Œæˆç»Ÿè®¡:")
    print(f"âœ… æ€»è®¡å·¥å…·: {len(tools_data)}")
    print(f"âœ… æŠ“å–æˆåŠŸ: {success_count}")
    print(f"âŒ æŠ“å–å¤±è´¥: {len(tools_data) - success_count}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_count/len(tools_data)*100:.1f}%")
    print(f"ğŸ“‹ å¹³å‡æ¯ä¸ªå·¥å…·æŠ“å–å­—æ®µ: {total_fields_scraped/success_count:.1f}" if success_count > 0 else "ğŸ“‹ å¹³å‡å­—æ®µ: 0")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    category_stats = {}
    for result in results:
        category = result['category']
        if category not in category_stats:
            category_stats[category] = {'total': 0, 'success': 0}
        category_stats[category]['total'] += 1
        if result.get('scraped_successfully'):
            category_stats[category]['success'] += 1
    
    print(f"\nğŸ“‚ æŒ‰ç±»åˆ«ç»Ÿè®¡:")
    for category, stats in category_stats.items():
        success_rate = (stats['success'] / stats['total']) * 100
        print(f"  {category}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")
    
    print(f"\nâœ¨ æŠ“å–å®Œæˆï¼")
    print(f"ğŸ“„ å®Œæ•´æ•°æ®: {output_file}")
    if successful_results:
        print(f"ğŸ“„ æˆåŠŸæ•°æ®: {success_file}")
    print(f"\nä¸‹ä¸€æ­¥ï¼šæ‚¨å¯ä»¥æŸ¥çœ‹æŠ“å–ç»“æœï¼Œç„¶åä½¿ç”¨WordPressè‡ªå®šä¹‰APIè¿›è¡Œå¯¼å…¥ã€‚")

if __name__ == "__main__":
    main() 