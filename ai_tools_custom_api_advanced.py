#!/usr/bin/env python3
"""
AIå·¥å…·å¯¼å…¥ - è‡ªå®šä¹‰WordPress APIå®¢æˆ·ç«¯ï¼ˆé«˜çº§ç‰ˆï¼‰
é›†æˆFirecrawlæ•°æ®æŠ“å–å’Œè‡ªå®šä¹‰WordPress APIå¯¼å…¥
"""

import requests
import json
import time
import logging
from requests.auth import HTTPBasicAuth
from typing import Dict, List, Optional
from csv_data_processor import CSVDataProcessor
from config import Config

class FirecrawlService:
    """FirecrawlæœåŠ¡å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.firecrawl.dev"):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def scrape_with_schema(self, url: str, schema: Dict) -> Optional[Dict]:
        """ä½¿ç”¨Schemaè¿›è¡Œç»“æ„åŒ–æŠ“å–"""
        try:
            payload = {
                "url": url,
                "formats": ["extract"],
                "extract": {
                    "schema": schema
                }
            }
            
            response = requests.post(
                f"{self.base_url}/v1/scrape",
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if data.get("success") and "extract" in data:
                    return data["extract"]
            
            return None
            
        except Exception as e:
            logging.error(f"FirecrawlæŠ“å–å¤±è´¥: {e}")
            return None

class AdvancedCustomAPIClient:
    """é«˜çº§WordPressè‡ªå®šä¹‰APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: Config):
        self.config = config
        self.base_url = f"{config.wordpress_url.rstrip('/')}/wp-json/ai-tools/v1"
        self.auth = HTTPBasicAuth(config.wordpress_username, config.wordpress_password)
        self.headers = {
            "Content-Type": "application/json",
            "User-Agent": "AI-Tools-Importer-Advanced/1.0"
        }
        
        # åˆå§‹åŒ–Firecrawl
        if config.firecrawl_api_key:
            self.firecrawl = FirecrawlService(config.firecrawl_api_key, config.firecrawl_base_url)
            self.schema = self.load_firecrawl_schema()
        else:
            self.firecrawl = None
            self.schema = None
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('advanced_api_import.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_firecrawl_schema(self) -> Dict:
        """åŠ è½½FirecrawlæŠ“å–Schema"""
        try:
            with open('ai_tool_firecrawl_schema.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½Firecrawl Schema: {e}")
            return {}
    
    def test_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        try:
            response = requests.get(
                f"{self.base_url}/test",
                auth=self.auth,
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                self.logger.info(f"âœ… WordPress APIè¿æ¥æˆåŠŸï¼ç”¨æˆ·: {data['user']['name']}")
                return True
            else:
                self.logger.error(f"âŒ WordPress APIè¿æ¥å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def scrape_tool_data(self, url: str) -> Optional[Dict]:
        """æŠ“å–AIå·¥å…·æ•°æ®"""
        if not self.firecrawl or not self.schema:
            return None
        
        try:
            self.logger.info(f"ğŸ” æ­£åœ¨æŠ“å–: {url}")
            data = self.firecrawl.scrape_with_schema(url, self.schema)
            
            if data:
                self.logger.info(f"âœ… æŠ“å–æˆåŠŸï¼Œè·å¾— {len(data)} ä¸ªå­—æ®µ")
                return data
            else:
                self.logger.warning(f"âš ï¸ æŠ“å–å¤±è´¥æˆ–æ— æ•°æ®: {url}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ æŠ“å–å¼‚å¸¸: {e}")
            return None
    
    def import_single_tool_with_scraping(self, tool_info: Dict) -> Dict:
        """ä½¿ç”¨æŠ“å–åŠŸèƒ½å¯¼å…¥å•ä¸ªAIå·¥å…·"""
        tool_name = tool_info.get('product_name', 'Unknown')
        tool_url = tool_info.get('product_url', '')
        
        # 1. æŠ“å–æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        scraped_data = {}
        if tool_url and self.firecrawl:
            scraped_data = self.scrape_tool_data(tool_url) or {}
        
        # 2. å‡†å¤‡å®Œæ•´æ•°æ®
        tool_data = self.prepare_complete_tool_data(scraped_data, tool_info)
        
        # 3. è°ƒç”¨APIå¯¼å…¥
        try:
            payload = {"tool_data": tool_data}
            
            response = requests.post(
                f"{self.base_url}/import",
                auth=self.auth,
                headers=self.headers,
                json=payload,
                timeout=120
            )
            
            if response.status_code in [200, 500]:
                result = response.json()
                result['scraped_fields'] = len(scraped_data)
                return result
            else:
                return {
                    "success": False,
                    "message": f"HTTPé”™è¯¯: {response.status_code}",
                    "errors": {"http": response.text},
                    "scraped_fields": len(scraped_data)
                }
                
        except Exception as e:
            return {
                "success": False,
                "message": f"è¯·æ±‚å¼‚å¸¸: {e}",
                "errors": {"exception": str(e)},
                "scraped_fields": len(scraped_data)
            }
    
    def batch_import_with_scraping(self, tools_list: List[Dict], batch_size: int = 5) -> Dict:
        """æ‰¹é‡å¯¼å…¥ï¼ˆå¸¦æŠ“å–åŠŸèƒ½ï¼‰"""
        total_tools = len(tools_list)
        all_results = []
        success_count = 0
        error_count = 0
        
        self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¯¼å…¥ {total_tools} ä¸ªAIå·¥å…·ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}ï¼‰")
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, total_tools, batch_size):
            batch_end = min(batch_start + batch_size, total_tools)
            batch_tools = tools_list[batch_start:batch_end]
            
            self.logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_start+1}-{batch_end}/{total_tools}")
            
            # ä¸ºå½“å‰æ‰¹æ¬¡å‡†å¤‡æ•°æ®
            prepared_batch = []
            for tool_info in batch_tools:
                tool_url = tool_info.get('product_url', '')
                
                # æŠ“å–æ•°æ®
                scraped_data = {}
                if tool_url and self.firecrawl:
                    scraped_data = self.scrape_tool_data(tool_url) or {}
                    time.sleep(1)  # é¿å…æŠ“å–è¿‡å¿«
                
                # å‡†å¤‡å®Œæ•´æ•°æ®
                tool_data = self.prepare_complete_tool_data(scraped_data, tool_info)
                prepared_batch.append(tool_data)
            
            # å‘é€æ‰¹æ¬¡è¯·æ±‚
            try:
                payload = {"tools": prepared_batch}
                
                response = requests.post(
                    f"{self.base_url}/batch-import",
                    auth=self.auth,
                    headers=self.headers,
                    json=payload,
                    timeout=600
                )
                
                if response.status_code == 200:
                    batch_result = response.json()
                    batch_results = batch_result.get("results", [])
                    
                    for result in batch_results:
                        if result.get("success"):
                            success_count += 1
                        else:
                            error_count += 1
                    
                    all_results.extend(batch_results)
                else:
                    # æ‰¹æ¬¡å¤±è´¥ï¼Œæ ‡è®°æ‰€æœ‰å·¥å…·ä¸ºå¤±è´¥
                    for tool_info in batch_tools:
                        all_results.append({
                            "tool_name": tool_info.get('product_name', 'Unknown'),
                            "success": False,
                            "message": f"æ‰¹æ¬¡è¯·æ±‚å¤±è´¥: {response.status_code}",
                            "errors": {"batch_http": response.text}
                        })
                        error_count += 1
                
            except Exception as e:
                # æ‰¹æ¬¡å¼‚å¸¸ï¼Œæ ‡è®°æ‰€æœ‰å·¥å…·ä¸ºå¤±è´¥
                for tool_info in batch_tools:
                    all_results.append({
                        "tool_name": tool_info.get('product_name', 'Unknown'),
                        "success": False,
                        "message": f"æ‰¹æ¬¡è¯·æ±‚å¼‚å¸¸: {e}",
                        "errors": {"batch_exception": str(e)}
                    })
                    error_count += 1
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if batch_end < total_tools:
                time.sleep(2)
        
        return {
            "success": True,
            "summary": {
                "total": total_tools,
                "success": success_count,
                "errors": error_count
            },
            "results": all_results
        }
    
    def prepare_complete_tool_data(self, scraped_data: Dict, original_data: Dict) -> Dict:
        """å‡†å¤‡å®Œæ•´çš„å·¥å…·æ•°æ®"""
        # å¼€å§‹åŸºæœ¬æ•°æ®
        tool_data = {
            "product_name": original_data.get('product_name', ''),
            "product_url": original_data.get('product_url', ''),
            "original_category_name": original_data.get('original_category_name', ''),
        }
        
        # åˆå¹¶æŠ“å–çš„æ•°æ®
        if scraped_data:
            # åŸºæœ¬å­—æ®µ
            basic_fields = [
                "short_introduction", "product_story", "primary_task", 
                "author_company", "general_price_tag", "initial_release_date"
            ]
            for field in basic_fields:
                if field in scraped_data and scraped_data[field]:
                    tool_data[field] = scraped_data[field]
            
            # å¸ƒå°”å­—æ®µ
            if "is_verified_tool" in scraped_data:
                tool_data["is_verified_tool"] = scraped_data["is_verified_tool"]
            
            # å›¾ç‰‡URL
            for img_field in ["logo_img_url", "overview_img_url"]:
                if img_field in scraped_data and scraped_data[img_field]:
                    tool_data[img_field] = scraped_data[img_field]
            
            # æ•°å€¼å­—æ®µ
            numeric_fields = [
                "popularity_score", "number_of_tools_by_author", 
                "average_rating", "rating_count"
            ]
            for field in numeric_fields:
                if field in scraped_data and scraped_data[field] is not None:
                    try:
                        tool_data[field] = float(scraped_data[field])
                    except (ValueError, TypeError):
                        pass
            
            # å¤æ‚å­—æ®µ
            if "pricing_details" in scraped_data and isinstance(scraped_data["pricing_details"], dict):
                tool_data["pricing_details"] = scraped_data["pricing_details"]
            
            # åˆ—è¡¨å­—æ®µ
            list_fields = [
                "inputs", "outputs", "pros_list", "cons_list", 
                "related_tasks", "releases", "job_impacts"
            ]
            for field in list_fields:
                if field in scraped_data and isinstance(scraped_data[field], list):
                    tool_data[field] = scraped_data[field]
        
        return tool_data

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = Config()
    
    print("\n=== AIå·¥å…·å¯¼å…¥ - é«˜çº§è‡ªå®šä¹‰WordPress APIå®¢æˆ·ç«¯ ===")
    config.display_summary()
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = AdvancedCustomAPIClient(config)
    
    # æµ‹è¯•è¿æ¥
    print("\nğŸ“¡ æµ‹è¯•WordPress APIè¿æ¥...")
    if not client.test_connection():
        print("âŒ WordPress APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ï¼")
        return
    
    # æ£€æŸ¥Firecrawlå¯ç”¨æ€§
    if client.firecrawl:
        print("âœ… FirecrawlæœåŠ¡å·²å¯ç”¨")
    else:
        print("âš ï¸ FirecrawlæœåŠ¡æœªé…ç½®ï¼Œå°†åªä½¿ç”¨CSVæ•°æ®")
    
    # åŠ è½½CSVæ•°æ®
    print("\nğŸ“ åŠ è½½CSVæ•°æ®...")
    processor = CSVDataProcessor(config.csv_file_path)
    tools_data = processor.get_tools_data()
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(tools_data)} ä¸ªAIå·¥å…·")
    
    # å¤„ç†æ•°é‡é™åˆ¶
    if config.debug_mode and config.max_tools_to_process:
        tools_data = tools_data[:config.max_tools_to_process]
        print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šé™åˆ¶å¤„ç† {len(tools_data)} ä¸ªå·¥å…·")
    
    # é€‰æ‹©å¯¼å…¥æ–¹å¼
    print("\nè¯·é€‰æ‹©å¯¼å…¥æ–¹å¼:")
    print("1. æ‰¹é‡å¯¼å…¥ï¼ˆæ¨èï¼‰")
    print("2. é€ä¸ªå¯¼å…¥")
    
    choice = input("è¾“å…¥é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        # æ‰¹é‡å¯¼å…¥
        batch_size = 5
        if client.firecrawl:
            batch_size = 3  # æŠ“å–æ¨¡å¼ä½¿ç”¨è¾ƒå°æ‰¹æ¬¡
        
        result = client.batch_import_with_scraping(tools_data, batch_size)
        
        summary = result.get("summary", {})
        print(f"\nâœ… æ‰¹é‡å¯¼å…¥å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡: {summary.get('total', 0)}")
        print(f"âœ… æˆåŠŸ: {summary.get('success', 0)}")
        print(f"âŒ å¤±è´¥: {summary.get('errors', 0)}")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        if config.debug_mode:
            results = result.get("results", [])
            for res in results:
                status = "âœ…" if res.get("success") else "âŒ"
                print(f"{status} {res.get('tool_name', 'Unknown')}: {res.get('message', 'No message')}")
    
    elif choice == "2":
        # é€ä¸ªå¯¼å…¥
        success_count = 0
        error_count = 0
        
        for index, tool in enumerate(tools_data, 1):
            print(f"\nğŸ“¦ å¤„ç†ç¬¬ {index}/{len(tools_data)} ä¸ªå·¥å…·: {tool['product_name']}")
            
            result = client.import_single_tool_with_scraping(tool)
            
            if result.get("success"):
                success_count += 1
                action = result.get("action", "processed")
                scraped_fields = result.get("scraped_fields", 0)
                print(f"âœ… {tool['product_name']} {action}æˆåŠŸ (æŠ“å–å­—æ®µ: {scraped_fields})")
                
                if result.get("warnings"):
                    print(f"âš ï¸ è­¦å‘Š: {len(result['warnings'])} ä¸ªéè‡´å‘½é”™è¯¯")
            else:
                error_count += 1
                scraped_fields = result.get("scraped_fields", 0)
                print(f"âŒ {tool['product_name']} å¤„ç†å¤±è´¥ (æŠ“å–å­—æ®µ: {scraped_fields})")
                print(f"   é”™è¯¯: {result.get('message', 'Unknown error')}")
                
                if config.debug_mode and result.get("errors"):
                    for error_type, error_msg in result["errors"].items():
                        print(f"   - {error_type}: {error_msg}")
            
            # æ·»åŠ å»¶è¿Ÿ
            if index < len(tools_data):
                time.sleep(2 if client.firecrawl else 1)
        
        print(f"\nğŸ“Š å¯¼å…¥å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main() 